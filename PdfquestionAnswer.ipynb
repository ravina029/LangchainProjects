{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAipeuMFmvoQ"
      },
      "source": [
        "# Querying pdf with Astra and Langchain\n",
        "\n",
        "Question Answering using Astra DB and Langchain,Powerd by Vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXnBqOMpnRDA"
      },
      "source": [
        "# Things to be done first\n",
        "1. need serverless cassandra with vector search database on  Astra DB, create database and get the database token with role database administrator and copy your database ID: and these parameters need momentarily\n",
        "\n",
        "2. we also need OpenAI API\n",
        "\n",
        "### we will do\n",
        "1. setup: import required deependaencies,provide secrets, create langchain vector store\n",
        "2. Run a. question answering loop to get the relevant info with LLM\n",
        "\n",
        "token\"AstraCS:EfPdbCYxHgXcWPARfwuuhuMx:bbfdf6678c238a2516af0887e39e0daa62779f50f16aab8bbbfc492767976fc2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqkWyRUhmo_I"
      },
      "outputs": [],
      "source": [
        "! pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KzCGoLCvqaRJ"
      },
      "outputs": [],
      "source": [
        "# Langchain comp to be used\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "#for dataset retrievial from hugingface\n",
        "from datasets import load_dataset_builder\n",
        "\n",
        "#with cassio the engine powering the Astra DB integration in Langchain, we will aslo initialize the DB connection\n",
        "\n",
        "import cassio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FpJEqga0rz5v"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCqA_NbWsMf4"
      },
      "source": [
        "# Required secrets\n",
        "1. AstraDB connection details and\n",
        "2. OpenAI API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N35iEVExsFhI"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  #as soon as we call it, it take environment variables from .env\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fdMaGuh8s2-6"
      },
      "outputs": [],
      "source": [
        "pdf_text=PdfReader('/Users/ravina/Desktop/langchainProjects/APJ Abdul Kalam Speech .pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gl2U8ioythTp"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Concatenate\n",
        "#read the text from given pdf\n",
        "raw_text=\"\"\n",
        "for i, page in enumerate(pdf_text.pages):\n",
        "  content=page.extract_text()\n",
        "  if content:\n",
        "    raw_text+=content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "DZY5d9Pbuday",
        "outputId": "191ba57e-9b45-404e-991c-08e88efe358b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thesis\\nRavina\\nMarch 2022\\n1Contents\\nAbstract\\nAbbreviations and Notations\\nIntroduction\\nPreliminaries and Background\\nAlgebraic Embedding\\nContinuously differentiable Embedding\\nSorting-based Embedding Schemes\\nExperimental observations\\nConclusion\\n21 Abstract\\nPermutation-invariant neural networks are gaining increasing popularity in Deep Learning. In this work, we\\nare particularly interested in the permutation invariant neural networks which are injective. such networks\\ncan recognize patterns in a set of inputs, even if the inputs are in different orders, without assuming that\\nthe inputs have a fixed size.\\nThese models can handle unordered input data and map each input to a unique output. This is particularly\\nuseful in tasks such as set-to-set or set-to-sequence learning where the input and the output data are un-\\nordered and unique mapping is required. However, the performance of these networks is highly sensitive to\\ninput perturbation. A small perturbation in the input can lead to a significant decrease in the accuracy of\\nthe model. Therefore, it is important to estimate the regularity of these network architectures for safe and\\naccurate practical applications. One of the key factors in determining the stability of deep neural networks\\nis the Lipschitz constant of these architectures. Generally, it is beneficial to constrain the function to be\\nBilipschitz which can be used to improve the stability and robustness of the model during training and\\nprevent it from overfitting to the training data. These models generalize well to the unseen data and have\\nsmooth well-behaved gradients that allow for effective optimization. Therefore, using a Bi-Lipschitz function\\ncan improve the robustness and stability of the model, leading to better performance and generalization.\\nIn this work, we analyzed the stability of separating permutation-invariant embedding schemes. first, we\\nproved that the power sum functions and in general every permutation invariant embedding obtained by the\\nsummation of a smooth or piecewise smooth function are not bilipschitz. On the other hand, it is already\\nknown that embeddings that combine linear mappings and 1-dimensional sorting are permutation invariant\\nand bi-Lipschitz and that the bi-Lipschitz constants can be characterized in terms of the singular values of\\nthe linear component of the embedding. In this work, we strengthen these results by proving injectivity with\\na significantly lower embedding dimension than what was previously known and improving the estimates\\nfor the Bi-Lipschitz constants. We performed several experiments to see the stability and accuracy of this\\nsorting-based embedding scheme corresponding to our estimate of the embedding dimension empirically. In\\neach experiment, we found that the general estimates of the Bi-lipschitz constant bound the experimental\\nfindings with respect to the randomly generated matrix Afor embedding in the higher dimensional space,\\nwhich shows the robustness of the model. Corresponding to these estimates of the embedding dimension the\\nmodel achieves stability and high accuracy. It achieves a hundred percent of accuracy with the increase in\\nthe embedding dimension (Relatively small embedding dimension proportional to n but not proportional to\\nfactorial n).\\n32 Abbreviations and notations\\n•N:the set of natural numbers .\\n•{1,2, . . . , n }:the set of natural numbers from 1 to n .\\n•|X|:the number of elements in set X .\\n•R:the set of real numbers\\n•Rd:set of vectors of length d\\n•Rd×n:set of all d×nmatrices\\n•(aij)n×n:n×nmatrix with element aij\\n•Sn:Symmetric group (set of all n×npermutation matrices)\\n•C∞:set of infinite differentiable functions\\n•max( S):the maximum of set S .\\n•min(S):the minimum of set S .\\n•n!:factorial of a natural number n\\n•|X|:number of elements in X\\n•[X]:Equivalence class of X\\n•Rn/Sn:Set of all the equivalence classes ∥X∥:Norm of X\\n•< X, Y > :Inner product of X and Y\\n•∼:equivalence relation\\n•↓:sorting in decreasing order\\n•M, m :Lipschitz constants\\n•σd:d-th singular value of matrix A\\n•σd,k:d-th singular value of d×dsub-matrix of Aindexed by k\\n•π:permutation element in Sn\\n•P:summation operator\\n•Conv (S):convex hull of a set S⊂Rd\\n•O(λn):of order n\\n•DSn:set of doubly stochastic matrices\\n•Ω:set notation\\n4•J(p):Jacobian of a function p\\n•≈:Approx\\n•i.e.:that is\\n•det(A):determinant of matrix A\\n•ker:kernel of a map\\n•rowsort (A):sorting the elements of a Arow-wise\\n53 Introduction\\nSeparating invariants and universality has been very important in machine learning and a lot of work has\\nbeen done on this. Our work is motivated by the problems of permutation invariant embedding schemes\\nwhich are separating(injective). An injective permutation invariant neural network is a type of network that\\ncan recognize patterns in a set of inputs, even if the inputs are in different orders, without assuming that\\nthe inputs have a fixed size. One example is the Deep Sets architecture proposed by [6] Zaheer et al. (2017)\\nwhich is based on the concept of set functions. Deep Sets apply the same neural network function to each\\nelement of a set and then aggregate the output using a permutation-invariant function, such as the sum or\\nthe mean. This architecture ensures that the output of the network is invariant to the order of the input\\nset elements. For instance, let’s say we want to classify a set of 2D points into two categories, ”circle” and\\n”square.” the Deep Sets architecture takes each 2D point in the set as input, uses a neural network to each\\npoint to generate a feature vector, and then applies a permutation-invariant function such as sum or mean\\nto the feature vectors to produce a single fixed-size output vector. The output vector is then fed into a final\\nclassification layer to classify the set as a ”circle” or ”square”. the injective property is important because it\\nensures that the output of the neural network is independent of the order in which the elements of the input\\nset are presented. This means that if we have two sets that contain the same elements but in a different\\norder, then this model will produce the same output for both sets.\\nTo avoid complexities in the neural networks, embeddings are significant because they allow representing\\nthe categorical or discrete data as continuous, high-dimensional vectors. We can capture the correlations\\nand similarities between distinct categories or words in this manner, which can be valuable for various tasks\\nsuch as natural language processing, computer vision, and recommender systems. Without embeddings, we\\nwould have to use one-hot encoding to represent categorical data, resulting in a sparse, high-dimensional\\nvector. Which is inefficient since it demands a large amount of memory and processing resources to process.\\nMoreover, it does not account for the links between various categories or terms. There is continuous work\\ngoing on to improve the different embedding schemes to get effective and efficient classification models. It is\\nmentioned in [1] that the separating algebraic embeddings for d= 1 are not Bi-Lipschitz even not Lipschitz,\\nford >1 they provided a way to construct a separating algebraic Lipschitz embedding. To add on, in this\\nwork, we provide proof that algebraic embeddings are not Bi-Lipschitz when d= 1. We proved that in\\ngeneral every smooth and piecewise smooth separating invariants are not Bi-Lipschitz for d≥1.\\nOur main work is focused on the problem which requires some conditions to be fulfilled by the considered\\nembedding scheme. which is formulated in the following theorem with αto be the considered embedding\\nscheme.\\nTheorem 3.1. There exist a function α:Rd×n→Rmwhich satisfy:\\n1.Invariance. For all X∼Ywe have α(X) =α(Y).\\n2.Separation. Ifα(X) =α(Y)then X∼Y.\\n3. the function αis bi-Lipschitz meaning that there exist 0< a 0< b 0such that\\na0d(X, Y)≤ ∥α(X)−α(Y)∥2≤b0d(X, Y)\\nfor all X, Y∈Rd×n\\nTo embed Rd×n/∼into some higher dimensional space, two embedding schemes are devised, based on sorting\\nand algebra of multivariate polynomial(Algebraic) [1]. We will discuss one by one both embedding schemes.\\n6In [5] it is shown that when d= 1, it is trivial to obtain separating invariant embeddings for the action\\nof permutation group SnonRd×nand it is an isometry therefore Bi-Lipschitz. For d >1 a set of separating\\ninvariants are suggested in [1] which are the composition of bilipschitz maps.\\nThe following theorem about the sorting-based embedding is proved in [1]. where they proved that the given\\nmap is Bi-Lipschitz for a very large value of the intermediate dimension D. and they provide the bounds on\\nthe lower and the upper Lipschitz constant. which can be summarised below.\\nTheorem 3.2. ForA∈RD×d,X∈Rd×n/Snandm= 2ndthe map βA,B:Rd×n/Sn−→Rmgiven by\\nβA,B(X) =B(βA(X))is a composition of two bilipschitz maps. Where, The map\\nβA:Rd×n/Sn−→RD×n\\ns.t\\nβA(X) =↓(AX)\\nwhere this map have the following properties.\\n(1)It is Bi-Lipschitz for any full spark matrix A∈RD×dwhen D= 1 + ( d−1)n!.\\n(2)Lower Lipschitz constant is given by the smallest d-th singular value of all the d×dsub-matrices of the\\nmatrix A.\\n(3)For any matrix A∈RD×dsuch that the map βA(X)defined above is injective then it is Bi-Lipschitz.\\nand the upper Lipschitz constant is given by the largest singular value of A.\\n(4)If for A∈RD×dthe map βA:Rd×n→RD×nis injective then for almost any linear map\\nB:RD×n→R2nd\\nthe map βA,B(X) =B(βA(X))is Bi-Lipschitz.\\nThis construction is separating when the embedding dimension Dis larger than the multiple of n!. But,\\na Large embedding dimension leads to an increase in the complexity of the model. Therefore, it is always\\ndesired to find a way to reduce the embedding dimension so that the space and time complexity of the model\\ncan be reduced. The mathematical problem discussed in this work is motivated by the above-mentioned\\nproblem where we observed that the embedding dimension of the above map is relatively unrealistically very\\nlarge and that the complexity can be reduced dramatically to a noticeable extent. It is proved in [3] that to\\nachieve the separation for the sorting-based embeddings, the required embedding dimension is 2 nd+1 or even\\nlower when the considered data manifold has a lower dimension. We proved that for D > n (d−1), sorting\\ncombined with a linear mapping βAis separating and Bi-Lipschitz. Also, we provide improved bounds for\\nthe Bi-Lipschitz constants.\\n3.1 Results:\\nOur main results can be summarised as follows.\\n1. For d= 1 the algebraic embeddings are not Bi-Lipschitz [1], in this work we proved this result. adding\\nto that we proved that in general, every smooth separating function is not bi-Lipschitz for d≥1. This\\nis also true for functions of the formnX\\ni=1ψi(x)\\neven if they are only piecewise-smooth.\\n72. For d= 1 it is trivial to obtain the separating invariant for the action of permutation group Snon\\nRd×n[5] . Adding to that we show that the action of SnonRnwhich corresponds to the separating\\nmap sorting is an isometry.\\n3. We provide the improved bounds for the Lipschitz constants of the map X7→rowsort (AX). And\\nshowed that the Bi-Lipschitz constants m(A) and M(A) satisfy\\nM(A) =σ1(A), and σd,k(A)≤m≤σd(A)\\nWhere σ1(A) is the largest singular value of Aandσd,k(A) is the d-th singular value of the d×d\\nsub-matrix of A.\\n4. Improved bounds for the Bi-Lipschitz constant of the above embedding lead to the improvement in the\\nstability of the model. We performed some Python experiments to see the variation in the Bi-Lipschitz\\nconstants of this embedding map with the variation in values of Dand found that the ratio of the\\nLipschitz constants stays bounded. That is the empirical values of the Lipschitz constants obtained\\nfrom the experiments remain within the bounds provided in the above result.\\n84 Preliminaries and Background\\nIn this section we will mention some of the results and theorem from [1]. We begin by providing some of the\\nimportant definitions and remarks used throughout our work.\\nDefinition 1. Permutation Invariant Function:\\nA function f:Rn→Rnis said to be permutation invariant if for all permutation π∈Sn\\nf(X1, X2, X3, ......X n) =f(Xπ(1), Xπ(2), Xπ(3), ......X π(n))\\nDefinition 2. ForX, Y∈Rd×n, X∼Yif there is some permutation matrix P∈Snsuch that X=Y P, a\\nmetric on Rd×n/∼is defined as follows:\\nd(X, Y) = min\\nP∈Sn∥X−Y P∥frobenius\\nDefinition 3. Sorting:\\nLetX∈Rnthen the map sorting ↓:Rn→Rnis given by ↓(x1, x2, . . . , x n) = (xp(1), xp(2), . . . , x p(n)). where\\nxp(1)≥xp(2)≥. . . ,≥xp(n).\\nDefinition 4. Full Spark matrix:\\nLetA∈RD×d, where D > d . If every d×dsubmatrix of Ais invertible or in particular of full rank then\\nthe matrix A is said to be a full spark matrix.\\nDefinition 5. Bi-Lipschitz Map:\\nForA∈RD×dan induced Map βA:Rd×n→RD×n,βA(X) =↓(XA) is said to be Bi-Lipschitz if there are\\nconstants mandMsuch that for all X, Y∈Rd×n,\\nm·d(X, Y)≤ ∥βA(X)−βA(Y)∥ ≤M·d(X, Y)\\nwhere dis defined in definition 3 and all the norms are Frobenious.\\nDefinition 6. Doubly Stochastic Matrix:\\nIt is a square matrix A= (aij) with non-negative real numbers each of whose rows and columns sums to\\none.\\nwe will use the following two theorems without providing their proof in our main result.\\nTheorem 4.1. Birkhoff-Von-Neumann:\\nIfAis a doubly stochastic matrix then there exist t1, t2, . . . , t n≥0,s.tPn\\ni=1ti= 1and permutation matrices\\nP1, P2, . . . , P nthen A=P1·t1+P2·t2+···+Pn·tn. such a decomposition of A is known as a convex\\ncombination.\\nTheorem 4.2. Caratheodory theorem:\\nLetS⊂Rd, ifx∈Conv (S)⊂Rdthen xis a convex sum of atmost d+ 1extremal points of S.\\nRemark: LetDSn={A∈Rn×n:Pn\\ni=1aij= 1,Pn\\nj=1aij= 1}be the set of all the doubly stochastic\\nmatrices. It is a compact convex polyhedron in Euclidean n2−space with dimension ( n−1)2and all the\\nn×npermutation matrices as the extreme points(vertices). (see,[4] A Survey of Matrix Theory and Matrix\\nInequalities. Allyn and Bacon).\\nThen using the theorem 4.1 and 4.2 every doubly stochastic matrix can be written as the convex combination\\nof atmost ( n−1)2+ 1 permutation matrices.\\nNext, we will prove the following lemma using definition 3.\\n9Lemma 4.3. let Ω n={(X1, X2, ....X n) :X1≤X2≤X3.......≤Xn} ⊂Rnthen for ddefined in definition\\n(3), we have\\nd(X, Y) =||X−Y||,∀X, Y∈Ωn............ (2)\\nProof. Let’s prove this for n= 2 and then for an arbitrary value of n.\\nCase1: for n=2\\nΩ2={(X1, X2) :X1≤X2}and suppose that X, Y ∈Ω2s.tX= (X1, X2)andY= (Y1, Y2)\\nFor the identity permutation, the result will hold. therefore, consider σ∈S2a non-identity permutation s.t,\\nYσ(1)=Y2and Y σ(2)=Y1.\\nSuppose to the contradiction that d(X, Y)̸=||X−Y||that is d(X, Y)≤ ||X−Y||. then we claim that,\\nYσ(2)≥Yσ(1).\\nLet’s simplify d(X, Y)≤ ||X−Y||\\n||X−Y||2−d(X, Y)2= (X1−Y1)2+ (X2−Y2)2−(X1−Yσ(1))2−(X2−Yσ(2))2\\n= (X1−Yσ(2))2+ (X2−Yσ(1))2−(X1−Yσ(1))2−(X2−Yσ(2))2....(using σ )\\n= 2X1(Yσ(1)−Yσ(2))−2X2(Yσ(1)−Yσ(2))\\n= 2(X1−X2)(Yσ(1)−Yσ(2))≥0 ( ∵d(X, Y)≤ ||X−Y||)\\n(1)\\nwhere ( X1−X2)≤0, therefore ( Yσ(1)−Yσ(2))≤0,this implies that ( Y2≤Y1). which is a contradiction with\\nthe definition of Ω. Thus the claim holds.\\n=⇒ ||X−Y||=d(X, Y)\\nCase2: n is arbitrary,\\nΩn={(X1, X2, X3......, X n) :X1≤X2≤X3...≤Xn} ⊆Rn\\nlet, X, Y∈Ωnand a non identity permutation σ∈Sns.tσ(i)̸=i.\\nIf,||X−Y||=d(X, Y)then\\n(X1−Y1)2+....+ (Xn−Yn)2= (X1−Yσ(1))2+.....+ (Xn−Yσ(n))2\\nConsider, ||X−Y|| −d(X, Y)>0\\nClaim: Yσ(j)≤Yσ(k)forj < k when σ(i)̸=i.\\nAssume Yσ(i)=Yi,∀i̸=j, k ....... (∗)\\nThen, ||X−Y|| −d(X, Y) = (X1−Y1)2..+ (Xn−Yn)2−(X1−Yσ(1))2−..(Xj−Yσ(j))2..(Xk−Yσ(k))2\\n......−(Xn−Yσ(n))2\\n= (Xj−Yj)2+ (Xk−Yk)2−(X(j)−Yσ(j))2−(Xk−Yσ(k))2...using (∗)\\n= 2Xj(Yk−Yj)−2Xk(Yj−Yk)\\n= 2(Xj−Xk)(Yk−Yj) ( Reduced to Case1 )\\n>0\\n(2)\\nfrom case 1, for n= 2, if ||X−Y|| −d(X, Y)>0 then Yj> Ykforj≤k. Therefore, Claim Yσ(k)≥Yσ(j)\\nholds for all j < k which implies Y /∈Ω. Therefore, for X, Y∈Ωn,||X−Y||=d(X, Y).\\n105 Algebraic Embedding(sum of the power function)\\nIn [1] polynomial embeddings (algebraic) are discussed by constructing their algebraic equivalent description\\nwhich is the sum of the power function given below.\\nSum of power function: First, Let’s construct a polynomial px(t) = ( t−x1)(t−x2). . .(t−xn) for\\nx∈Rni.e.x= (x1, x2. . . x n). Let’s write it in the form px(t) =tn+a1(x)tn−1+···+an(x). The algebraic\\nequivalent description of p(x) given by symmetric polynomials can be written using Newton-Girard identities\\nand Vieta’s formula, which is\\nf:Rn→Rn,f(x) = nX\\nl=1xl,nX\\nl=1x2\\nl···nX\\nl=1xn\\nl!\\nNext, we analyze the algebraic embedding scheme to see whether this scheme satisfies all the properties\\nmentioned in theorem 3.1.\\nLemma 5.1. The sum of the power function f:Rn→Rnmentioned above is permutation invariant and\\nseparating on Rn/Sn.\\nProof. Clearly, this map is invariant to the permutation of the input elements because summation doesn’t\\ndepend on the ordering of the elements.\\nTo see that fseparates the points of Rn/Sn, letx, y∈Rn/Snsuch that x̸=ythen we need to show that\\nf(x)̸=f(y). as x̸=ytherefore, there exists at least one index jsuch that xj̸=yj. Consider, for x̸=y,\\nand we have f(x) =f(y)i.e.\\nf(x)−f(y) = nX\\nl=1(xl−yl),nX\\nl=1(x2\\nl−y2\\nl),···nX\\nl=1(xn\\nl−yn\\nl)!\\n= 0\\nComparing both sides we can write,nX\\nl=1(x3\\nl−y3\\nl) = 0\\nby assumption taking the j-th index we have, x3\\nj−y3\\nj= (xj−yj)(x2\\nj+xj·yj+y2\\nj)\\nAs,xj̸=yjtherefore, xj−yj̸= 0 and for any possible xj̸=yjthe expression ( x2\\nj+xj·yj+y2\\nj) doesn’t\\nvanish. Therefore,Pn\\nl=1(x3\\nl−y3\\nl)̸= 0 which contradicts the assumption that f(x) =f(y).\\ntherefore, x̸=y=⇒f(x)̸=f(y) which shows that the above map is separating on Rn/Sn.\\nWe will show that despite the fact that this embedding is separating and invariant to the permutation,\\nit is not bilipschitz.\\nLemma 5.2. The sum of the power function p:Rn→Rnis not Bi-Lipschitz and thus doesn’t satisfy all\\nthe properties mentioned in problem 3.1.\\nProof. The Power sum function p:Rn→Rnis defined as follows:\\np(x) = nX\\ni=1xi,nX\\ni=1x2\\ni, . . . ,nX\\ni=1xn\\ni!\\n11from lemma 5.1 it is clear this map is permutation invariant and separating. To check whether the above\\nfunction is bi-Lipschitz, let’s take a metric on Rn/∼as we defined in definition 3, i.e.\\nd(X, Y) = min\\nσ∈SnvuutnX\\ni=1(xi−yσ(i))2 (3)\\nThe function pis bi-Lipschitz with the constant M≥m > 0 if,\\nm≤||p(X)−p(Y)||\\nd(X, Y)≤M\\nWhere,\\nM= sup\\nX,Y∈Rnd(X,Y )̸=0||p(X)−p(Y)||\\nd(X, Y)(4)\\nm= inf\\nX,Y∈Rnd(X,Y )̸=0||p(X)−p(Y)||\\nd(X, Y)(5)\\nJacobian of the function pis given by:\\nJ(p) =\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 1 . . . 1\\n2x1 2x2. . . 2xn\\n............\\nnxn−1\\n1 nxn−1\\n2 . . . nxn−1\\nn\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nThe determinant of the above Jacobian J(p) will be zero if xi=xjfor any i̸=j. Ifxi∈(xj−λ, xj+λ)\\nin the neighbourhood of xjfor some λ >0 then xi→xjwhen λ→0. Therefore By the continuity of the\\nfunction p,det(J(p))≈0.\\nIn particular, Let X= (x1, x2, .....x n)s.t x 1=x2and the other coordinates are different from each\\nother. Then the Jacobian matrix of the function p(X) reduced to:\\nJp(X)=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 1 1 . . . 1\\n2x1 2x1 2x3. . . 2xn\\n............\\nnxn−1\\n1 nxn−1\\n1 nxn−1\\n1 . . . nxn−1\\nn\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nAs two columns of the above matrix are identical therefore det(Jp(X)) = 0. due to the zero determinant, the\\nmatrix Jp(X)has nonempty null space.\\nLet, 0 ̸=a= (a1, a2, ......a n)∈ker(J(p(X))). then,\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 1 . . . 1\\n2x1 2x1. . . 2xn\\n............\\nnxn−1\\n1 nxn−1\\n1 . . . nxn−1\\nn\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8×\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8eda1\\na2\\n...\\nan\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed0\\n0\\n...\\n0\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n12taking the point Xλ=X−λ.ain the neighborhood of X, below the line at which point X lies.\\nApplying Taylor’s expansion on the function of n variables, we have:\\np(Xλ) =p(X−λ.a)\\n=p(X)−λJp(X).a+O(∥(λ.a)∥2)\\np(Xλ)−p(X) =λJp(X).a+O(λ2)\\n=⇒p(Xλ)−p(X) =O(λ2)as J p(X).a= 0(6)\\nasX, X λ∈Ωn, therefore by lemma 4.3 we have:\\nd(Xλ, X) =|Xλ−X|and,|Xλ−X| ∼λ.|a|\\nThus we can write the expression in (6) as:\\nm= inf\\nXλ,X∈Rn|Xλ−X|̸=0||p(Xλ)−p(X)||\\n|Xλ−X|∼λ2\\nλ−→0as λ −→0\\nThus, m→0 asλ→0. Therefore the function p:Rn−→Rnis not Bi-Lipschitz. As this embedding scheme\\nis not bilipschitz and thus not suitable to embed the input data due to stability issues of this map.\\nThe above map is not Bi-Lipschitz however in [1] they mentioned that using a technique presented in\\n[2] this map can be transformed to Lipschitz but not bilipschitz. Also in [1] they devised a methodology\\nto construct the algebraic embedding for dimension d > 1. In this work, we will not discuss the higher-\\ndimensional algebraic embeddings.\\nBut, we will prove that every general permutation invariant function that is separating and continuously\\ndifferential is not Bi-Lipschitz for the dimensions( d≥1). First, we will prove this result for d= 1.\\n136 Continuously differentiable Functions\\nProposition 6.1. Letf:Rn−→Rmbe any general function such that:\\n(1)fisSninvariant.\\n(2)fseparates points i.e, if f(X) =f(Y)then [X] = [Y]\\n(3)f∈C∞(Rn,Rm)i.e.fis continuously differentiable.\\nthen fis not Bi-Lipschitz\\nProof. LetX∈Rns.tX1=X2i.eX= (X1, X1, X3.....X n), define a Linear function,\\nh:R−→Rn, s.th(t) = (X1−t, X 1+t, X 3, X4, ........X n)\\nWe can write\\nf(X1, X1, .....X n) =f◦h(0) and f◦h(t) =f(X1−t, X 2+t, X 3, X4, ........X n).\\nSince fisSninvariant, therefore, f◦h(t) =f◦h(−t)\\nClaim:\\natX= (X1, X1, X3.....X n)∈Rn, fort̸= 0 the Lipshitz constant\\nm= inf\\nXλ,X∈Rn|Xλ−X|̸=0||f(Xλ)−f(X)||\\n|Xλ−X|\\n= inf\\nXλ,X∈Rn|Xλ−X|̸=0||f◦h(0 +λ.t)−f◦h(0)||\\n|h(0 +λ.t)−h(0)|(7)\\nWhere,\\nd(h(0 +λ.t), h(0)) = |h(0 +λ.t)−h(0)|\\n=|(X1−λ.t, X 1+λ.t, X 3.....X n)−(X1, X1, X3.....X n)|\\n=|(λ.t, λ.t, 0,0, ......0)|\\n=√\\n2.λ.t=O(λ)(8)\\nTo simplify the numerator in (5), define a function, ψ(t) :R−→Rns.t.ψ(t) =f◦(h(t))\\n∵fisSninvariant, therefore, ψ(t) =f◦(h(t)) =f◦(h(−t) =ψ(−t))\\nforλ∈R, the Taylor series expansion of ψaround 0.\\nψ(−λ·t) =ψ(0−λ·t) =ψ(0)−λ·t·(ψ′(0)) + O(||λ·t||2) (9)\\nψ(λ·t) =ψ(0 +λ·t) =ψ(0) + λ·t·(ψ′(0)) + O(||λ·t||2) (10)\\nasψ(t) =ψ(−t) therefore, by solving the above equations we have ψ′(0) = 0\\nTherefore, ψ′(0) = ( f◦h(0))′= 0, substituting this in above equation we have,\\nψ(0 +λ·t)−ψ(0) = O(||λ·t||2)\\ni.e.\\n||f◦h(0 +λ.t)−f◦h(0)||=O(λ2)\\nTherefore,\\nm= min\\nXλ,X∈Rn|Xλ−X|̸=0||f◦h(0 +λ.t)−f◦h(0)||\\n|h(0 +λ·t)−h(0)|=O(λ2)\\nO(λ)−→0as λ −→0\\n14thus, for the general function satisfying the given conditions, m→0 asλ→0 which implies these functions\\nare not Bi-Lipschitz.\\nTheorem 6.2. Letf:Rd×n−→Rmbe a function such that:\\n(1)fisSninvariant.\\n(2)fis separating points i.e, if f(X) =f(Y)then [X] = [Y]\\n(3)f∈C∞(Rn,Rm)i.e.fis continuously differentiable.\\nthen fis not Bi-Lipschitz\\nProof. For an arbitrary d∈Nlet’s consider a function g:R−→Rd×ns.t.\\ng(t) =\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edX1−t X 1+t X 3. . . X n\\nX1−t X 1+t X 3. . . X n\\n............\\nX1−t X 1+t X 3. . . X n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nwhere each of the columns in the above matrix is constant and Xi≤Xi+1.\\nUsing the above function, we can write f(X1, X1, ........X n) =f◦g(0)\\nf(X1−t, X 1+t, X 3, ......X n) =f◦g(t)\\nLetXt=g(t) for some infinitesimal small t >0.\\nandX0=g(0) = ( X1, X1, X3, X4, ........X n) where Xidenote each of the columns and Xi≤Xi+1.\\nnow using these XtandX0in the following expression we obtained,\\nm= inf\\nX,Y∈Rd×n|X−Y|̸=0||f(X)−f(Y)||\\n|X−Y|\\n≤Xt,X0∈Rd×n|Xt−X0|̸=0||f◦(g(0 +t))−f◦(g(0))||\\n|g(0 +t)−g(0)|(11)\\nAsXi≤Xi+1, therefore using the lemma 4 .3 for each i-th row, we have d(Xt(i), X0i) =|Xt(i)−X0i|\\nd(g(0 +t), g(0)) = |(X1−t, X 1+t, X 3.....X n)−(X1, X1, X3.....X n)|\\n=|(−t, t,0,0, ......0)|\\n=√\\n2dt....... (∵Xi∈Rd)(12)\\nNow to simplify the numerator let’s define a function:\\nψ:Rd×n−→Rm\\nsuch that\\nψ(t) =f◦g(t)\\nSince fisSninvariant, therefore, ψ(−t) =f◦(g(−t)) =f◦(g(t)) =ψ(t)\\nfort∈R, the Taylor series expansion of ψaround 0 is:\\nψ(−t) =ψ(0−t) =ψ(0)−t·(ψ′(0)) + O(||t||2) (13)\\nψ(t) =ψ(0 +t) =ψ(0) + t·(ψ′(0)) + O(||t||2) (14)\\n15asψ(t) =ψ(−t) therefore, by solving above equations we have ψ′(0) = 0\\nSubstituting ψ′(0) = 0 in (15), we have:\\nψ(0 +t)−ψ(0) = O(||t||2)\\n=k·t2(k∈R)(15)\\nusing expressions (14) and (16),\\nm≤Xt,X0∈Rd×n|Xt−X0|̸=0||f◦(g(0 +t))−f◦(g(0))||\\n|g(0 +t)−g(0)|\\n=k·t2\\n√\\n2dt→0as t→0.(16)\\nBut m is a positive constant. therefore, m→0 ast→0.\\nIf a given function is bilipschitz then by the definition the bilipschitz constants mandMmust be non-zero.\\nBut in this case, we obtained m= 0, Therefore the considered function is not bilipschitz for every point in\\nthe given domain. which implies every smooth invariant and separating function is not Bi-Lipschitz.\\nNext, we analyze the behavior of the piece-wise smooth functions of the form f(X) =Pn\\ni=1ψi(X) which\\nare permutation invariant and separating. we will show that these functions are also not Bi-Lipschitz. which\\ncan be proved using the above result. Let’s state this in the following theorem.\\nTheorem 6.3. Every piece-wise smooth function of the form f(X) =Pn\\ni=1ψi(X)which separates points\\nand, invariant to the permutation is not Bi-Lipschitz.\\nProof. As the given function f:Rd×n→Rnis piece-wise smooth. Then, for every X∈Rd×nthere is a\\ncomponent ψiwhich is smooth in the neighbourhood of X.\\nUsing the previous lemma, for the function ψithe Lipschitz constant mi→0 in the neighborhood of all the\\npoints where ψiis defined.\\nEstimate for the Lipschitz constant of the given function is given by m= min i∈[1,2...,n]mi→0.\\ntherefore by the definition of the Bi-Lipschitz function, mmust be non zero but here it’s zero. therefore, a\\npiece-wise smooth function is not bi-lipschitz.\\nFrom the above theorem and the previous one, it is clear that every smooth and piecewise smooth\\nfunctions are not Bi-Lipschitz. therefore embedding with the above functions is not robust to the perturbation\\nin the input data.\\nTherefore, We need an embedding scheme that is robust to even small perturbations in the input data.\\nIn the next, we’ll analyze the sorting-based embedding schemes whether they are Bi-Lipschitz or not.\\n167 Sorting-based Embedding Schemes\\nBelow is the description of the sorting map defined on Rn.\\nDefinition 7. (Sorting ) Sorting denoted by Sort arranges the elements of input in increasing or decreasing\\norder. For the permutation group Snthe map\\nSort :Rn\\nSn−→Rn,\\nis defined as, Sort(X) = (X1, X2, X3, ......X n) s.t, X1≤X2≤X3, ......≤Xn.\\nwhere Xis the representative of [ X]∈Rn\\nSn.\\nRemark: By definition sorting is permutation invariant and separates the points of Rn/Sn.\\nNext, we will prove that for the dimension d= 1, Sorting is an isometry. Let’s formulate this in the\\nfollowing proposition.\\nProposition 7.1. The map Sort :Rn\\nSn−→Rn,defined above is an isometry. That is, for X, Y∈Rn\\nSn, and a\\nmetric d defined in definition 2, following holds,\\nd(X, Y) =∥sort(X)−sort(Y)∥\\nProof. Let Ω n={(X1, X2, ....X n) :X1≤X2≤X3.......≤Xn} ⊂Rnbe the set such that components of\\neach element are in the increasing order. let Xbe the representative of [ X]∈Rn\\nSn, then Sort(X)∈Ωn.\\nby 2nd definition,\\nd(X, Y) =minσ∈SnvuutnX\\nj=1|Xj−Yσ(j)|2\\nLet,Sort(X) =X∗, where X∗∈Ωn\\nthen,∥sort(X)−sort(Y)∥=∥X∗−Y∗∥and by Lemma 4.3, d(X, Y) =∥X∗−Y∗∥\\ntherefore, using these two expressions we have d(X, Y) =∥sort(X)−sort(Y)∥\\nThus, it is clear that the distance is preserved under the Sorting map which means it is an isometry. Therefore\\nbeing an isometry Sorting is a bilipschitz where the input space is one dimensional.\\nFrom the above remark and proposition, it is clear that the map Sort :Rn\\nSn−→Rn,satisfies all the\\nconditions of the theorem 3.1, that is Sorting is separating, permutation invariant, and isometry(bilipschitz).\\nNext, we will analyze sorting combined with the linear map given the input space is of a dimension greater\\nthan one. where the underlying map is βA:Rd×n/Sn−→RD×nsuch that βA(X) =↓(AX).\\nClearly, with A∈RD×dthe sorting combined with linear map i.e. βA(X) is permutation invariant. In the\\nfollowing theorem, we will show that the above map is separating. later on, we will prove that this map is\\nBi-Lipschitz too.\\nTheorem 7.2. LetA∈RD×dandX∈Rd×n/SnandD > n (d−1). then the sorting(in the decreasing\\norder) combined with linear mapping\\nβA:Rd×n/Sn−→RD×n\\nβA(X) =↓(AX)\\nis separating.\\n17Proof. Fix D,d s.t D≥d≥2 and let A∈RD×dis generic meaning that all the d×dsubmatrices of A are\\nof rank d.\\nconsider n=n(D, d) be the smallest such that X, Y∈Rd×nare not equal upto permutation, but rowsort (AX) =\\nrowsort (AY). By assumption, each column Xiis not equal to any column Yj. otherwise, we would obtain\\na contradiction to the minimality of n. This implies that each pair AXiandAYjhave at most d-1 entries in\\ncommon(since A is generic).\\nFixi. Each of the D entries of AXiis equal to the entries of one of AYjj= 1,2,3.....n asrowsort (AX) =\\nrowsort (AY). This implies D≤n(d−1).which is not possible as D > n (d−1).Therefore the map βAis\\nseparating.\\nLet’s now discuss the stability of βAby studying its bi-Lipschitz constants. To do so we introduce the\\nfollowing notation: For I⊆ {1, . . . , D }we denote A(I) = (Aij)i∈I,1≤j≤d. We denote the d-th singular value\\nofA(I) byσd(A(I)) and for k≥dwe define\\nσd,k(A) = min\\nI⊆{1,...,D},|I|=kσd(A(I)).\\ndenote the maximal singular value of Abyσ1(A).\\nTheorem 7.3. IfA∈RD×dis full spark and k≥d≥2, D≥k\\x02\\n(n−1)2+ 1\\x03\\n, then\\n∥βA(X)−βA(Y)∥F≥σd,k(A)d(X, Y)\\nProof. Let’s consider the matrix X, Y∈Rd×nand we can denote ¯X=AX, ¯Y=AY. Let i-th row and the\\nj-th column of the matrix X∈Rd×nis denoted by ¯Xi∗and ¯X∗jrespectively. There exist permutations\\nσ1, . . . , σ D∈Snsuch that\\n∥βA(X)−βA(Y)∥2\\nF=DX\\ni=1∥sort(¯Xi∗)−sort(¯Yi∗)∥2\\n2=DX\\ni=1nX\\nj=1|¯Xij−¯Yiσi(j)|2\\n=nX\\nj=1nX\\nℓ=1X\\ni∈Ij,ℓ|¯Xij−¯Yiℓ|2(17)\\nwhere, the notation\\nIj,ℓ={i|σi(j) =ℓ}.\\nNote that for fixed jwe haveP\\nℓ|Ij,ℓ|=D, and for fixed ℓwe haveP\\nj|Ij,ℓ|=D. We deduce\\nLemma 7.4. There exists a permutation σ∈Snsuch that\\n|Ijσ(j)| ≥d,∀j= 1, . . . , n\\nProof. LetS∈Rn×nbe defined by Sjℓ=|Ijℓ|. Then1\\nDSis a doubly stochastic matrix, that is, its entries\\nare all non-negative and its rows and columns sum to one. By the Birkhoff-Von-Neumann Theorem coupled\\nwith the Caratheodory Theorem, we can write1\\nDSas a convex combination of N= (n−1)2+1 permutation\\nmatrices\\n1\\nDS=NX\\ns=1θsP(s),\\n18where θiare non-negative and sum to one. Let tbe such that θt≥θs,∀s. It follows that θt≥1\\nN. Let σbe\\nthe permutation corresponding to P(t), such that\\nP(t)\\njℓ=\\x1a1 if ℓ=σ(j)\\n0 if otherwise\\nthen we have for all j\\n0≤1\\nDSjσ(j)−θtP(t)\\njσ(j)=1\\nDSjσ(j)−θt≤1\\nDSjσ(j)−1\\nN\\nand so for all j,\\n|Ijσ(j)|=Sjσ(j)≥D\\nN≥k\\nReturning to (17) we obtain\\nnX\\nj=1nX\\nℓ=1X\\ni∈Ij,ℓ|ˆXij−ˆYiℓ|2≥nX\\nj=1X\\ni∈Ij,σ(j)|¯Xij−¯Yiσ(j)|2\\n=nX\\nj=1∥A(Ij,σ(j))Xj−A(Ij,σ(j))Yσ(j)∥2\\n≥σd,k(A)2nX\\nj=1|Xj−Yσ(j)|2\\n≥σd,k(A)2d2(X, Y)\\nCombining this inequality with the previous theorem, we concluded\\n∥βA(X)−βA(Y)∥F≥σd,k(A)d(X, Y)\\nTheorem 7.5. LetA∈RD×dandX∈Rd×n/Sn. then the mapping βA:Rd×n/Sn−→RD×ngiven by\\nβA(X) =↓(AX)\\nsatisfies,\\nm∥X−Y∥ ≤ ∥ βA(X)−βA(Y)∥F≤M∥X−Y∥.\\nWhere M=σ1(A), the spectral norm of Aandσd,k(A)≤m≤σd(A). Also, there exist constant vectors X\\nand Y for which there is equality on the right side.\\nProof. Note that the considered map βAis permutation invariant therefore, βA(X) =βA(PX) for any\\npermutation P∈Snand for any X∈Rd×n/Sn. first, we will show that, ∥βA(X)−βA(Y)∥ ≤M∥X−Y∥:\\nLetX, Y∈Rn\\nSns.td(X, Y)̸= 0. Let a1. . . a Ddenotes the rows of the matrix A. That is A=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1\\na2\\n...\\naD\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fbthen there\\n19exist the permutation matrices P∗, P1, P2. . . P DandΠ1. . .ΠD∈Snsuch that, For any X, Y∈Rd×n/Snwe\\nhave\\nd(X, Y) =∥X−Y P∗∥frobenius\\nand\\nβA(X) =↓A(X) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1XP 1\\na2XP 2\\n...\\naDXPD\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fbandβA(Y) =↓A(Y) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1YΠ1\\na2YΠ2\\n...\\naDYΠD\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nTherefore,\\n∥βA(X)−βA(Y)∥2=DX\\ni=1∥aiXPi−aiYΠi∥2=DX\\ni=1∥ai(XPiΠT\\ni−Y)∥2\\nAlso, by the optimality condition of the permutations, we have\\n∥aiXPi−aiYΠi∥=min P∈Sn∥aiX−aiY P∥2\\nTherefore, we have\\n∥aiXPi−aiYΠi∥ ≤ ∥ aiX−aiY P∗∥2\\nThus\\n∥βA(X)−βA(Y)∥2≤DX\\ni=1∥aiX−aiY P∗∥2=∥A(X−Y P∗)∥2≤σ1(A)2∥X−Y P∗∥\\nwhere M=∥A∥op=σ1(A).Therefore,\\n∥βA(X)−βA(Y)∥2≤M\\nThis bound is attained when the considered matrices are constant. Let’s evaluate it by taking X∗andY∗\\nsuch that X∗= (X, X............, X ) and Y∗= (Y, Y.........., Y ). where X, Y denote the constant columns of\\nX∗andY∗. Since X∗is constant. therefore, βA(X∗) =↓(AX∗) =AX∗\\nThus,\\n∥βA(X∗)−βA(Y∗)∥2=∥A(X∗)−A(Y∗)∥2\\n=∥A(X∗−Y∗)∥2\\n=∥A∥2\\nopnX\\ni=1∥(X−Y)∥2\\n=n∥A∥2\\nop∥X−Y∥2\\nThe distance between the above selected constant vectors is given by,\\nd(X∗, Y∗)2=min P∈Sn∥X∗−PY∗∥2\\n=n∥X−Y∥2\\nop\\nas each of the entries in both the vectors X∗andY∗is constant therefore the matrix Pin the above\\nexpression can be any permutation matrix on nsymbols. Using this observation we obtained,\\n∥βA(X∗)−βA(Y∗)∥=∥A∥opd(X∗, Y∗)\\n=M×d(X∗, Y∗)\\n20where, M=∥A∥op=σ1(A).\\nTherefore for the constant vectors the inequality on the right side in the given expression is in fact equality.\\nLet,\\nm=infX̸∼Y∥βA(X)−βA(Y)∥F\\n∥X−Y∥\\nthen, for some constant vector X̸∼Y∈Sn, a minimum of the above expression is attained as we have\\nshown for maximum. therefore, we have\\nm=infX̸∼Y∥βA(X)−βA(Y)∥F\\n∥X−Y∥(18)\\n≤min X∗̸∼Y∗∥βAX∗−βAY∗)∥F\\n∥X∗−Y∗∥(19)\\nWhere, X∗, Y∗are the constant matrices with the columns XandYas considered before. All the considered\\nnorms are Frobenius. writing,\\nm2≤min X∗̸∼Y∗constant∥βAX∗−βAY∗)∥2\\n∥X∗−Y∗∥2=min∥A(X∗)−A(Y∗)∥2\\n∥X∗−Y∗∥2(20)\\n=minn∥A(X−Y)∥2\\nn∥X−Y∥2(21)\\n=minn(X−Y)T×ATA×(X−Y)\\nn(X−Y)T×(X−Y)(22)\\n=min∥A∥2= (σd)2(23)\\nWhere σdis the d-th singular value of A. Thus, m≤σd.\\nAlso, from theorem 7.3 for the full spark matrix A∈RD×dandk≥d≥2, D≥k\\x02\\n(n−1)2+ 1\\x03\\n, we have\\n∥βA(X)−βA(Y)∥F≥σd,k(A)d(X, Y).\\nTherefore, using theorem 7.3 and the expression m≤σdwe have σd,k≤m≤σd. Thus\\nm∥X−Y∥ ≤ ∥ βA(X)−βA(Y)∥F≤M∥X−Y∥.\\nsuch that M=σ1(A), and σd,k(A)≤m≤σd(A).\\n218 Experimental Observations:\\nWe performed some experiments to observe the stability and accuracy of the binary classification model\\nbased on the above sorting-based embedding scheme.\\nStability of the model: To check the stability of the model empirically, we analyzed the ratio M/m\\nof the maximum and minimum values of the Lipschitz constants of the embedding map βA(X) =↓(AX)\\nwhich embed the input data in RD×nspace. In this experiment, our code doesn’t use any specific data set\\nrather it randomly generates input data in the space Rd×n. To calculate the Lipschitz constants of the above\\nfunction we took randomly generated matrix A∈RD×dwhich embed the input data in the D−dimensional\\nspace, and a permutation matrix P∈Rn×nis taken to perform the sorting operation. The magnitude of\\nthe difference of the function with respect to different values is calculated using the frobenious norm. We\\nrepeated the experiment several times therefore in each experiment the random matrix A is created using the\\nseed method and then utilized throughout the experiment with various values of the varying parameter. The\\ncode conducts the same experiment with different random matrices Acorresponding to different seed values.\\nwhich is useful to assess how sensitive the outcomes are to the matrix selection. we performed the experiment\\nby taking Acorresponding to ten different seeds and for each seed, the experiment was repeated 10 times.\\nFor every seed, we obtained the ratio of M/m corresponding to the considered values of d, D, and n , and\\nfinally, we took the average of all these ratios w.r.t to all the ten seeds. This experiment gives the following\\nobservations.\\n•Variation in the stability w.r.t to the number of points nin the point cloud: In this Experiment,\\nwe fixed the embedding dimension Dand dimension of the input dataset dat 3 and performed the\\nexperiment for each n∈ {5,10,15,20,25,30,35,40,45,50}. In this setting the values of M/m w.r.t n\\nare shown in the fig1, for the small value of nthe ratio M/m is quite high more than 1 .9 but as the\\nnumber nincreases to 40 this ratio drops abruptly to less than 1 .5. for values more than n= 40, it\\nremains below 1.4. Therefore for the large number of points in the input point cloud, this embedding\\nscheme is quite stable. we will see the accuracy of the model for this case in later experiments.\\nIn the Next experiment, we fixed the size of the input point cloud and plot the variations w.r.t other\\nparameters. which is described in the following paragraph.\\n•Variation in the stability w.r.t to input dimension d: In this Experiment, we fixed the number of points\\nin the input point cloud at n=1024 and the embedding dimension at D= 3. The ratio M/m for each\\nd∈ {2,3,4,5. . .15}is calculated and the results of this experiment are plotted in the graph3 in fig1.\\nfrom this graph, it is observed that With the increase in the values of dthe ratio M/m shows a sudden\\nincrease and decrease with an irregular zig-zag pattern. From the graph, it is clear that the increase or\\nchange in the dimension of the input data set leads to the instability of the model. Therefore if there\\nis a change in the input dimension the model will not be robust.\\nTo fix this problem we fixed the input dimension in the next experiment to see the change with respect\\nto the embedding dimension and obtained the following results.\\n•Variation in the stability w.r.t to the parameter D: We fixed the number of points in the point cloud\\natn= 1024 and input dimension at d= 3. With this setting the value of ratio M/m for each of\\nD∈ {5,10,20,30,40,50,60,70,80}is plotted in the graph5 of fig1. For the small value of Dthe ratio,\\nM/m is more than 1 .65 and it decreases abruptly for 5 < D < 20. After that, except for a small\\nincrease, there is a decrease in the ratio as the value of Dincreases and attains a value less than 1 .5\\nfor all the values of D > 30. This experiment shows the improvement in the stability of the model\\n22with the increase in the embedding dimension. Therefore, the model will be stable if we fix the values\\nofnandd. Next, we will experiment to analyze the accuracy of the model by changing the different\\nparameters.\\nFigure1 stability and accuracy of sorting based embedding\\n23Accuracy of the model: To check the accuracy of the model we performed an experiment for the task\\nof binary classification to see, how accurately this model classifies the given data. For this task of binary\\nclassification, we took the same setting as in the previous experiment. A SortNet neural network consisting\\nof three fully connected layers is trained on a data set of point clouds generated from two classes of lines in\\na 3-dimensional space, representing either squares or triangles. We used the Relu activation function in the\\nmiddle layers and logsoftmax in the output layer. Then the considered model is tested on the test data to\\nsee how correctly it can classify the given data in the category to which it belongs.\\nIn fig1, along with the stability graphs, there are graphs of accuracy.\\nWe saw that with the increase in the size of the input point cloud, the ratio M/m decreases i.e. the\\nstability of the model increases. But, from the experiment, we found that the accuracy of the model doesn’t\\nimprove with the change in the size of the input dataset which is shown in graph2 (fig1). In fact, it decreases\\nwith the increase in the size of the input dataset. Therefore, the size of the input data must be fixed to\\nachieve good accuracy.\\nIn the next experiment, we took the variation in the input dimension. In this case again similar to the\\nstability the accuracy of the model is uncertain, which is depicted in Graph4 (fig1). As we saw when the\\ndimension of the input point cloud was not fixed the model was not stable. similarly, the graph of accuracy\\nwith respect to d shows an irregular increase and decreases in the accuracy of the model. Therefore to attain\\ngood accuracy we must fix the dimension of the input data.\\nIn this experiment, we fixed the size n and dimension d of the input data. We took the variations in the\\nembedding dimension Dand observed that there is a continuous improvement in the accuracy of the model.\\nFor the embedding dimension D < 20 the model achieves an accuracy of more than 99 percent. Further for\\nall the values of D > 20, the model achieves an accuracy of 100 percent, which is depicted in graph 6 of the\\nfig1.\\nNote:\\n1. The learning rate of the model is fixed at 0.001 for each accuracy experiment.\\n2. The values in the graph are average over six experiments performed by varying the parameters.\\nTherefore, Empirically we found that the embedding dimension plays an important role in the stability and\\naccuracy of the model but within certain bounds which are in the range we proved theoretically. Thus the\\nmodel will be robust and give high accuracy if the embedding dimension will lie within the specified bounds\\nwith the fixed size point cloud and the fixed input dimension.\\n24Conclusion: We analyzed the stability and accuracy of several embedding schemes. To ensure the\\nstability of the considered embedding scheme we checked whether these are bilipschitz or not. It is observed\\nthat the Algebraic embedding schemes are permutation invariant, separating, and in this work, we proved\\nthat these embedding schemes are not Bi-Lipschitz and thus not stable for small changes in the input data.\\nTherefore, these embedding schemes are not robust for small variations in the input data.\\nSorting-based embedding on one-dimensional space is permutation invariant, separating, and Bi-Lipschitz.\\nFor higher dimensional space, we analyzed the sorting combined with linear mapping which is permutation\\ninvariant, and also Bi-Lipschitz but for a higher embedding dimension D. In this work, we managed to\\nreduce this embedding dimension to an extent that is very less compared to what is already known to us.\\nWe showed that for D > n (d−1) this embedding scheme is separating. It is already known that the upper\\nLipschitz constant is approximated by the first singular value of the matrix Athat is M=σ1(A), the\\nspectral norm of A, and the lower Lipschitz constant is given by the d-th singular value of A. In this work\\nwe proved the improved bounds on the lower Lipschitz constant m, that is σd,k(A)≤m≤σd(A), where\\nσd(Ais the d-th singular value of Aandσd,k(A) is the σd,k(A) = min I⊆{1,...,D},|I|=kσd(A(I)) over all the\\nd×dsub-matrices of A.\\nThus, we proved that with a very small embedding dimension compared to the previous known value the\\nsorting-based embedding scheme is robust to the small variations in the input dataset. To verify this\\nempirically we performed several python experiment and observed that the stability of the model increases\\nwith the increase in the number of sample points ( n) keeping D= 3andd= 3 fixed. To examine the\\naccuracy of this embedding scheme we trained a SortNet model to classify the Triangles and lines. We\\nobserved that in this case the model shows an accuracy of more than 82 percent. Also, as we increased the\\nembedding dimension keeping the other parameters nanddfixed the stability of the model increases with the\\nincrease in the value of D. The accuracy of the SortNet model gradually increases and attains the accuracy\\nof 100 percent for D≥30 for the fixed values of n= 1024 andd= 3.\\nFrom the experiments we observed that for the size of the dataset and fixed input dimension the model with\\nthe sorting based embedding scheme is Robust and give high accuracy.\\n25References\\n[1] Radu Balan, Naveed Haghani, and Maneesh Singh. Permutation invariant representations with applica-\\ntions to graph deep learning. arXiv preprint arXiv:2203.07546 , 2022.\\n[2] Jameson Cahill, Andres Contreras, and Andres Contreras-Hip. Complete set of translation invariant\\nmeasurements with lipschitz bounds. Applied and Computational Harmonic Analysis , 49(2):521–539,\\n2020.\\n[3] Nadav Dym and Steven J Gortler. Low dimensional invariant embeddings for universal geometric learning.\\narXiv preprint arXiv:2205.02956 , 2022.\\n[4] Marvin Marcus. A survey of matrix theory and matrix inequalities; university allyn and bacon. Inc.:\\nBoston, MA, USA , 1964.\\n[5] Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, and Michael A Osborne. On the\\nlimitations of representing functions on sets. In International Conference on Machine Learning , pages\\n6487–6494. PMLR, 2019.\\n[6] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\\nAlexander J Smola. Deep sets. Advances in neural information processing systems , 30, 2017.\\n26'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF3A-dzWumoV"
      },
      "source": [
        "# Initialize the connection to your database:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBxXSVPjuhAg",
        "outputId": "662619e5-c9b0-43ba-bb3e-3c77054e43f0"
      },
      "outputs": [],
      "source": [
        "#cassio.init(token=os.getenv('ASTRA_DB_APP_TOKEN'),database_id=os.getenv('ASTRA_DB_ID'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SVF0DLBvBGq"
      },
      "source": [
        "# create Langchain embeddings and llm objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HwZV9_vDu5sZ"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
        "embedding=OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_WlBLfYvloN"
      },
      "source": [
        "# create the langchain vector store backed by astra DB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NR8RyVbivibw"
      },
      "outputs": [],
      "source": [
        "astra_vector_store=Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"my_pdf_qna\",\n",
        "    session=None,\n",
        "    keyspace=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MDH6Yx1PwWTH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# using  CharacterTextSplitter to devide the text into chunk so that it shouldn't increase the token size\n",
        "\n",
        "text_splitter=CharacterTextSplitter(\n",
        "    separator='\\n',\n",
        "    chunk_size=700,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "texts=text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgbdVVUCxeZH"
      },
      "outputs": [],
      "source": [
        "texts[:30] #top 30 chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XucZMsP8z1lr"
      },
      "source": [
        "# Load the dataset into vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjId02JLzytY",
        "outputId": "4a9281bf-7cc8-4cd2-e5cf-8444302d17c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inserted 50 lines.\n"
          ]
        }
      ],
      "source": [
        "astra_vector_store.add_texts(texts[:50])\n",
        "print(\"inserted %i lines.\" %len(texts[:50]))\n",
        "astra_vector_index=VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYNaE3Rs0naz"
      },
      "source": [
        "# Let's ask questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKispVJ4xiWz",
        "outputId": "6e7160ce-2bc5-47af-8ce5-dafe709515b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\Enter your question (or type 'quit to exit):what is the topic of this document\n",
            "\n",
            "Question: \\what is the topic of this document\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "answer:'The topic of this document is about permutation-invariant neural networks with injective properties and their applications in deep learning.'\n",
            "\n",
            "First document by relevnce:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8795] \"Thesis\n",
            "Ravina\n",
            "March 2022\n",
            "1Contents\n",
            "Abstract\n",
            "Abbreviations and Notations\n",
            "Introduc ...\"\n",
            "[0.8724] \"problem where we observed that the embedding dimension of the above map is relat ...\"\n",
            "[0.8682] \"sub-matrix of A.\n",
            "4. Improved bounds for the Bi-Lipschitz constant of the above e ...\"\n",
            "[0.8655] \"the categorical or discrete data as continuous, high-dimensional vectors. We can ...\"\n",
            "\\what's your next question (or type 'quit to exit):provide the 10 line summry of this document\n",
            "\n",
            "Question: \\provide the 10 line summry of this document\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "answer:'This document discusses permutation-invariant neural networks and their use in Deep Learning. The focus is on injective networks that can recognize patterns in inputs regardless of their order or size. The problem of embedding dimension is addressed and it is shown that sorting combined with a linear mapping can reduce complexity. Polynomial embeddings and their algebraic equivalent description are also discussed. The Deep Sets architecture is explained, which uses a permutation-invariant function to classify sets of inputs. The importance of the injective property is emphasized.'\n",
            "\n",
            "First document by relevnce:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8663] \"Thesis\n",
            "Ravina\n",
            "March 2022\n",
            "1Contents\n",
            "Abstract\n",
            "Abbreviations and Notations\n",
            "Introduc ...\"\n",
            "[0.8621] \"problem where we observed that the embedding dimension of the above map is relat ...\"\n",
            "[0.8599] \"In [1] polynomial embeddings (algebraic) are discussed by constructing their alg ...\"\n",
            "[0.8587] \"the mean. This architecture ensures that the output of the network is invariant  ...\"\n",
            "\\what's your next question (or type 'quit to exit):quit\n"
          ]
        }
      ],
      "source": [
        "first_question=True\n",
        "while True:\n",
        "  if first_question:\n",
        "    query_text=input(\"\\Enter your question (or type 'quit to exit):\").strip()\n",
        "  else:\n",
        "    query_text=input(\"\\what's your next question (or type 'quit to exit):\").strip()\n",
        "\n",
        "  if query_text.lower()==\"quit\":\n",
        "    break\n",
        "\n",
        "  if query_text.lower()==\"\":\n",
        "    continue\n",
        "\n",
        "  first_question=False\n",
        "\n",
        "  print(\"\\nQuestion: \\%s\\\"\" %query_text)\n",
        "  answer=astra_vector_index.query(query_text,llm=llm).strip()\n",
        "  print(\"answer:\\'%s\\'\\n\"%answer)\n",
        "\n",
        "  print(\"First document by relevnce:\")\n",
        "  for doc,score in astra_vector_store.similarity_search_with_score(query_text,k=4):\n",
        "    print(\"[%0.4f] \\\"%s ...\\\"\"% (score,doc.page_content[:100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZlT00TV3HQl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
